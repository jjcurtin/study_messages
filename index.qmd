---
title: Evaluating Cellular Communication Sensing for Lapse Risk Prediction During Early Recovery from Alcohol Use Disorder
author:
  - name: Kendra Wyant
    email: kpaquette2@wisc.edu
    orcid: 0000-0002-0767-7589
    corresponding: false
    #roles: []
    affiliations:
    - Department of Psychology, University of Wisconsin-Madison
  - name: Coco Yu
    email: jyu274@wisc.edu
    orcid: 0000-0002-7731-0563
    corresponding: false
    #roles: []
    affiliations:
    - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    orcid: 0000-0002-3286-938X
    corresponding: true
    email: jjcurtin@wisc.edu
    #roles:
      #- Project administration
      #- Software
      #- Visualization
    affiliations:
    - Department of Psychology, University of Wisconsin-Madison
keywords:
  - Substance use disorders
  - Machine learning
  - Cellular Sensing
abstract: |
#plain-language-summary: |
  #To be filled in.
#key-points:
  #- Take away point 1 
  #- Take away point 2
date: last-modified
citeproc: true
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
bibliography: messages.bib
#citation:
  #container-title: To be filled in. 
number-sections: false 
tbl-cap-location: bottom
editor_options: 
  chunk_output_type: console
---

<!--
Target Journal: JMIR Formative Research (1500 words)

Goal word counts: 

- Introduction: 350 words (currently 432 words)
- Methods: 600 words (currently 613 words)
- Results: 200 words (currently 194)
- Discussion: 350 words (currently 383)

total: 1622 words
-->

```{r}
#| echo: false
#| message: false

library(tidyverse)
suppressPackageStartupMessages(source("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true"))

path_models <- format_path("risk/models/messages")

pp_perf <- read_csv(here::here(path_models, "pp_perf_tibble.csv"),
                    show_col_types = FALSE)

contrast <- read_csv(here::here(path_models, 
                                "contrast_baseline.csv"),
                    show_col_types = FALSE)
```


# Introduction

Alcohol Use Disorder (AUD) is a chronic, relapsing disease [@mclellanDrugDependenceChronic2000;@dennisManagingAddictionChronic2007; @rounsavilleLapseRelapseChasing2010]. Lapses, single episodes of alcohol use, are among the strongest predictors (and a necessary precursor) for relapse, a full return to harmful drinking [@marlattRelapsePreventionMaintenance1985; @marlattRelapsePreventionSecond2007]. While lapses can occur at any point in recovery, they are particularly risky during early recovery [@daleyReducingRiskRelapse2019]. Protective coping mechanisms and socio-environmental resources that support recovery are dynamic and accumulate over time [@clevelandRecoveryRecoveryCapital2021]. As a result, early recovery represents a critical window of vulnerability during which a lapse is more likely to escalate into relapse.

An automated recovery support system powered by personal sensing and machine learning may assist with the inherently difficult task of identifying when and why someone is at increased risk for lapse. Personal sensing of densely sampled data from individuals' day-to-day lives can provide the inputs necessary for temporally dynamic lapse predictions [@mohrPersonalSensingUnderstanding2017]. Early machine learning models using ecological momentary assessment data have achieved excellent accuracy [@chihPredictiveModelingAddiction2014; @wyantMachineLearningModels2024; @wyantForecastingRiskAlcoholunderreview]. Still, questions remain about the long-term feasibility of self-report sensing methods and whether new, important risk factors might emerge from sensing methods that passively collect smartphone data without user input.
<!--not sure what you mean by the "whether new important risk factors might emerge from passs... "  Are you thinking about the limitation that EMA is limited to what we already know as risks?  Maybe another sentence or two to make that clear.   Also, I think that you might have two paragraphs here.  One that talks about how we can already predict well with EMA and a second that discusses why we need other methods (both burden) and your idea of exploration of other factors we dont measure with ema-->

Cellular communication sensing may be one promising method. It offers the potential for greater temporal specificity for capturing fluctuations in risk compared with self-report data (i.e., as it occurs vs. prompting users to make reports the next day<!--this is a bit confusing.   Maybe highlight that EMA is limited to at most several assessment per day but communications can be monitorined moment by moment-->). Late night phone calls could indicate an emergency, "drunk dialing", or interpersonal conflict. It can also capture risk-relevant constructs difficult for people to self report, such as slow changing social patterns. A decrease in the number of contacts someone is communicating with could indicate a shrinking social circle, isolation, or disengagement.

These passive data may become more powerful still when communications are contextualized with personal meaning for a given participant (e.g., Who is this contact to them?). In the above examples, contextualized communication data might reveal that late-night phone calls are to a sponsor or that a shrinking social circle is due to reduced contact with people unsupportive of their recovery.
<!--this previous paragraph needs another couple of sentences or examples to make clear what you mean-->

In this study, we evaluated the performance of a machine learning model that predicts the probability of a next-day alcohol lapse among individuals in early recovery from AUD using contextualized cellular communication data. We also describe the most important features contributing to these predictions, with the goal of identifying new, clinically meaningful features emerging from communication-based sensing.
<!--this one should also be fleshed out a bit more.   Make it focus on the stregth of the predictive signal in features dervied from celluar communications.   It sounds like you are more intested in the model than the features as written.-->

# Methods

## Participants and Procedure
We recruited adults in early recovery from AUD in Madison, Wisconsin, through print and digital advertisements and partnerships with treatment centers. Eligibility criteria required that participants were age 18 or older, able to read and write in English, had moderate to severe AUD ^[(≥4 self-reported DSM-5 symptoms)], had been abstinent from alcohol for 1–8 weeks, were willing to use a single smartphone, and were not exhibiting severe psychosis or paranoia.^[Defined as scores >2.2 or 2.8, respectively, on the psychosis or paranoia scales of the Symptom Checklist–90 [@derogatislBriefSymptomInventory].]

Participants completed up to 5 study visits over approximately 3 months: a screening visit, intake visit, and 3 monthly follow-up visits. At screening we collected demographic information (age, sex at birth, race, ethnicity, education, marital status, employment, and income) and clinical characteristics (DSM-5 AUD symptom count, alcohol problems [@hurlbutAssessingAlcoholProblems1992], and presence of psychological symptoms [@derogatislBriefSymptomInventory]). At intake we collected additional self-report data on abstinence self-efficacy [@mckiernanDevelopmentBriefAbstinence2011], craving [@flanneryPsychometricPropertiesPenn1999], and recent recovery efforts. At each monthly follow-up, we downloaded cellular communication metadata (voice calls and SMS text message logs) from participants' smartphones. We identified important contacts (i.e., individuals they had communicated with at least twice by call or text in the past month) and asked 7 contextual questions about these contacts.

While enrolled, participants completed 4 brief daily ecological momentary assessments (7-10 questions). The first item assessed alcohol use (date and time of any unreported drinking episodes). Lapse reports were verified at follow-up visits using a timeline follow-back interview. Additional sensing data streams and self-report measures were collected for the parent grant. The full study protocol is available on our Open Science Framework page ([https://osf.io/wgpz9/](https://osf.io/wgpz9/)). 

We screened 192 participants. Of these, 169 enrolled and 154 completed the first follow-up. Data from 10 participants were excluded due to loss of abstinence goals, careless responding, or unusually low compliance. The final analytic sample included 144 participants.
<!--what is loss of abstinence goals?  Couldnt we use their data until their gaol changed?  Earlier, you didnt say they had to have an abstineenc goal per se, just that they were abstintent for 1-8 weeks.  Also, low compliance on what?-->

## Data Analysis Plan
Our models predicted the probability of an alcohol lapse within a 24-hour window. Predictions were generated daily at 4 a.m., beginning on participants' second study day and continuing for up to 3 months. In total, there were 11,507 labeled prediction windows across all participants. 

Features were engineered from all available data up to the start of each window.^[We filtered the data to include only communications with known context prior to feature engineering.] The full model included 406 features from cellular communication data plus 24 features from baseline self-report measures. We also evaluated a comparison model that used only the baseline features. @tbl-1 details the raw predictors and feature engineering procedures. 
<!--table 1 and text doesnt give enough detail about feature engineering I think.  At a minimum, its not clear what is meant by difference vs. raw features.  Also not clear what a scoring epoch is?-->
<!--A bit more is needed on the outcome.   You talk about predictions but not the label itself-->
<!--I dont think I understand context enough from the text and table either.  We need to know how it was collected and on who (only frequent contacts).   We then need to know that numbers are linked to these contacts so you can count them but that the features that dont use context are for all contacts, regardless if they are a frequent person-->
Candidate model configurations differed by algorithm (elastic net, random forest, XGBoost), outcome resampling method, and hyperparameter values. The best configuration for each model was selected using 6 repeats of participant-grouped 5-fold cross-validation. Our performance metric was area under the receiver operating curve (auROC). Folds were stratified by a between-subject measure of our outcome (low lapsers: 0-9 lapses; high lapsers: 10+ lapses).   
<!--the statification sentence is confusing.  Dont link to outcome directly,  Just say stratified such that all folds contained comparable proportions of individuals who lapsed frequent (10 + times).-->
We evaluated model performance with a Bayesian hierarchical generalized linear model. Posterior distributions with 95% credible intervals (CI) were estimated from the 30 held-out test sets using weakly informative, data-dependent priors to regularize and reduce overfitting.^[Residual SD ~ normal(0, exp(2)); intercept (centered predictors) ~ normal(2.3, 1.3); window-width contrasts ~ normal(0, 2.69); covariance ~ decov(1,1,1,1).] Random intercepts were included for repeat and fold (nested within repeat). auROCs were logit-transformed and regressed on model type to estimate the probability that model performances differed systematically. 

Our best performing models used an elastic net algorithm. We quantified feature importance by examining the retained features (i.e., coefficient value > 0) in the full model and ordering them by absolute coefficient value. These values provide an estimate of the direction and magnitude of association between each predictor and the outcome, conditional on the other features retained. All our annotated analysis scripts are publicly available on our study website ([https://jjcurtin.github.io/study_messages/](https://jjcurtin.github.io/study_messages/)).

{{< embed notebooks/mak_tables.qmd#tbl-1 >}} 

<!--Coco: It feels a little weird to have the last two columns. How about separating the tables so that we have one for baseline-only and one for additional communication features?-->

## Ethical Considerations
All procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study #2015-0780). All participants provided written informed consent.


# Results

## Participants
@tbl-2 provides the demographic characterization of our sample. We obtained a total of 375,912 contextualized communications across participants. Participants had, on average, 2,610 communications (range = 109-14,225). 56% of participants reported at least one lapse. 

{{< embed notebooks/mak_tables.qmd#tbl-2 >}} 

## Model Evaluation
The median posterior auROC for the full model was `r round(subset(pp_perf, model == "full model")$pp_median, 2)`, with relatively narrow 95% CI ([`r round(subset(pp_perf, model == "full model")$pp_lower, 2)`, `r round(subset(pp_perf, model == "full model")$pp_upper, 2)`]) that did not contain .5. This provides strong evidence that the model is capturing signal in the data. The final model retained 13 features (@fig-1). The top four were baseline measures of abstinence confidence, having a goal of abstinence, abstinence self-efficacy when experiencing negative affect, and craving. Communication frequency with people unaware of the individual's recovery goals also emerged as an important feature associated with increased lapse risk.

We evaluated a comparison model to assess the incremental predictive value of cellular communication features beyond baseline measures. The baseline model retained 5 features and achieved performance nearly identical to the full model (median auROC = `r round(subset(pp_perf, model == "baseline model")$pp_median, 2)`, 95% CI [`r round(subset(pp_perf, model == "baseline model")$pp_lower, 2)`, `r round(subset(pp_perf, model == "baseline model")$pp_upper, 2)`]). The median difference in auROC between the full and baseline models was less than .01, providing no evidence (52% probability) that their posterior distributions were meaningfully different.
<!--I think this is the probasbilityh that the full model performed better than the baseline model.  Bit surprised that its even .52 given how close they auROCs are. Are you sure about that?-->

<!-- Retained communication features:
Interesting they are all over the longer period durations - features become more meaningful with more data?

p168.l0.dratecount.phone_number,
p72.l0.rratecount.drink_status.NonDrinker,
p168.l0.rratecount.drink_status.Drinker,
p168.l0.rratecount.support_status.Dont Know,
p168.l0.rratecount.contact_experience.Unpleasant,
p168.l0.rratecount.cont_type.friend   
-->

{{< embed notebooks/mak_figures.qmd#fig-1 >}} 




# Discussion

Our model achieved fair performance, with an auROC of `r round(subset(pp_perf, model == "full model")$pp_median, 2)`, indicating that some predictive signal was present. However, it did not offer incremental value beyond a baseline model that included only demographic and self-report measures. Consistent with this, the four most important predictors in our model were all self-report variables: abstinence confidence, abstinence goal, negative affect efficacy, and craving. 

Nonetheless, several communication features were retained in the final model with moderately sized coefficients. These included communications with people unaware of the participant's recovery status, non-drinkers, friends, and individuals who were unpleasant to interact with. In contrast, raw counts of calls and text messages and call durations were not retained in the final model. This implies that the quantity of communication may be less informative than the quality and social significance. Future research may benefit from collecting richer contextual data about communication contacts to better understand the social dynamics contributing to lapse risk. 

Even with highly contextualized communication data, however, prediction may be limited by data sparsity. Many participants had few daily communications, and some had extended periods with no recorded interactions at all. Our study design may have further contributed to this limitation. We collected only phone and SMS text communications through the native smartphone app. In recent years, many individuals use private messaging apps (e.g., WhatsApp, Signal) or social media platforms (e.g., Facebook Messenger, Instagram) as their primary communication method [@mcdowellPreferencesAttitudesDigital2025]. Therefore, our dataset likely missed a substantial portion of participants' communications. Future studies could explore whether data from these platforms yield stronger predictive signals.
<!--I think you need to provide data/results in the results section to support this sparsity issue-->
<!--you should probabably have also provided some detail about the context info.  How many frequent contacts do p eople have?  How many of there individual commuinications are with frequent contacts.  etc-->

We cannot entirely dismiss the potential value of cellular communication data for risk prediction. For example, researchers have successfully incorporated communication data into models with other sensing data (e.g., accelerometer, geolocation, and device usage) to predict alcohol use episodes [@baeDetectingDrinkingEpisodes2017; @baeLeveragingMobilePhone2023]. However, even in these instances, the contribution of cellular communications is questionable <!--what makes it questionable?  need to be clearer about that-->and other sensing methods like geolocation appear to be more promising <!--why?-->. Other practical challenges for collecting call and text message data further limit the feasibility of this sensing method (e.g., Apple heavily restricts the collection of these data by apps in their app store <!--how did we get them then?   Method should make this clear-->). We conclude that other forms of social interaction characterization (e.g., engineering time spent with supportive contacts from geolocation data) are more worthwhile to pursue in future research.



\newpage
