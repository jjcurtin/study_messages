---
title: Evaluating Cellular Communication Sensing for Lapse Risk Prediction During Early Recovery from Alcohol Use Disorder
author:
  - name: Kendra Wyant
    email: kpaquette2@wisc.edu
    orcid: 0000-0002-0767-7589
    corresponding: false
    #roles: []
    affiliations:
    - Department of Psychology, University of Wisconsin-Madison
  - name: Coco Yu
    email: jyu274@wisc.edu
    orcid: 0000-0002-7731-0563
    corresponding: false
    #roles: []
    affiliations:
    - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    orcid: 0000-0002-3286-938X
    corresponding: true
    email: jjcurtin@wisc.edu
    #roles:
      #- Project administration
      #- Software
      #- Visualization
    affiliations:
    - Department of Psychology, University of Wisconsin-Madison
keywords:
  - Substance use disorders
  - Machine learning
  - Cellular Sensing
abstract: | 
  Alcohol Use Disorder (AUD) is a chronic, relapsing disease. An automated recovery support system powered by personal sensing and machine learning may assist with the inherently difficult task of identifying when and why an individual is at increased risk for lapse. Cellular communication sensing may be one promising method for detecting dynamic changes in lapse risk. Moreover, these communications can be contextualized with self-reported, risk-relevant information about the contact. In this study, we evaluated the performance of a machine learning model that predicts the probability of next-day alcohol lapse among individuals (N = 144) in early recovery from AUD using contextualized cellular communication data and baseline demographic and AUD characteristics. The best-performing model used an elastic net algorithm and retained 13 features. The median posterior auROC for the best model was 0.68, with a relatively narrow 95% Bayesian confidence interval (CI; [0.64, 0.71]) that did not include 0.5. We also evaluated a comparison model to assess the incremental predictive value of cellular communication features beyond baseline measures. The baseline model retained five features and achieved performance nearly identical to that of the full model (median auROC = 0.68, 95% CI [0.64, 0.71]). These results indicate that cellular communication data capture some risk-relevant signal for alcohol lapse; however, they do not provide incremental predictive value beyond a baseline model that includes only demographic and self-report measures. Several communication features were retained in the final model with moderately sized coefficients, suggesting that aspects of social communication may be important for understanding lapse risk. Nonetheless, limitations inherent to cellular communication as a sensing method may outweigh their added value.
#plain-language-summary: |
  #To be filled in.
#key-points:
  #- Take away point 1 
  #- Take away point 2
date: last-modified
citeproc: true
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
bibliography: messages.bib
#citation:
  #container-title: To be filled in. 
number-sections: false 
tbl-cap-location: bottom
editor_options: 
  chunk_output_type: console
---

<!--
Target Journal: JMIR Formative Research (1500 words)


total: 1788 words without intro
-->

```{r}
#| echo: false
#| message: false

library(tidyverse)
suppressPackageStartupMessages(source("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true"))

path_models <- format_path("risk/models/messages")

pp_perf <- read_csv(here::here(path_models, "pp_perf_tibble.csv"),
                    show_col_types = FALSE)

contrast <- read_csv(here::here(path_models, 
                                "contrast_baseline.csv"),
                    show_col_types = FALSE)
```


# Introduction

Alcohol Use Disorder (AUD) is a chronic, relapsing disease [@mclellanDrugDependenceChronic2000;@dennisManagingAddictionChronic2007; @rounsavilleLapseRelapseChasing2010]. Lapses, single episodes of alcohol use, are among the strongest predictors (and a necessary precursor) for relapse, a full return to harmful drinking [@marlattRelapsePreventionMaintenance1985; @marlattRelapsePreventionSecond2007]. While lapses can occur at any point in recovery, they are particularly risky during early recovery [@daleyReducingRiskRelapse2019]. Protective coping mechanisms and socio-environmental resources that support recovery are dynamic and accumulate over time [@clevelandRecoveryRecoveryCapital2021]. As a result, early recovery represents a critical window of vulnerability during which a lapse is more likely to escalate into relapse.

An automated recovery support system powered by personal sensing and machine learning may assist with the inherently difficult task of identifying when and why someone is at increased risk for lapse. Personal sensing of densely sampled data from individuals' day-to-day lives can provide the inputs necessary for temporally dynamic lapse predictions [@mohrPersonalSensingUnderstanding2017]. Early machine learning models using ecological momentary assessment (EMA) data have achieved excellent accuracy in predicting future lapses back to alcohol use in treatment seeking populations [@chihPredictiveModelingAddiction2014; @wyantMachineLearningModels2024; @wyantForecastingRiskAlcoholunderreview]. 

Despite the high predictive success of EMA, questions remain about the long-term feasibility of self-report sensing method. EMA has been shown to be well-tolerated among substance use populations over relatively short periods of time [@jonesComplianceEcologicalMomentary2019; @wyantAcceptabilityPersonalSensing2023]. It is unclear whether individuals would be willing and able to adhere to an extensive EMA protocol (e.g., 4 prompts per day) indefinitely. Moreover, EMA items are chosen using domain expertise from decades of research on the self-report factors that predict lapse. It is possible, however, that there are several alterntive precipitators of lapse not yet discovered due to small subtle changes in one's environment, social circle, or lifestyle that can be easily pinpointed in self-report. 

Cellular communication sensing may be a promising alternative to EMA. Whereas EMA is limited to at most several assessments per day, communication sensing is mostly passive and can be monitored moment-by-moment. Cellular communication patterns also appear to capture clear, risk-relevant constructs. Late-night phone calls could indicate an emergency, “drunk dialing,” or interpersonal conflict. A decrease in the number of contacts an individual communicates with could reflect a shrinking social circle, isolation, or disengagement.

These data may become even more powerful when communication patterns are contextualized with participant-specific meaning. For instance, knowing a participant’s relationship to their contacts, whether they have previously drank alcohol with a given contact, or whether that contact supports their recovery goals could substantially alter interpretation. In the examples above, contextualized communication data might reveal that late-night calls are made to a sponsor, or that a shrinking social circle reflects reduced contact with individuals unsupportive of their recovery. In this way, the same communication patterns may reflect protective processes rather than increased lapse risk.

In this study, we assessed whether contextualized cellular communication features contain clinically meaningful signals for predicting next-day alcohol lapse risk among individuals in early recovery from AUD. Using a machine learning model, we evaluated the predictive utility of these features and identified the most important communication features, with the goal of uncovering new, clinically meaningful predictors of lapse risk.


# Methods

## Participants and Procedure
We recruited adults in early recovery from AUD in Madison, Wisconsin, through print and digital advertisements and partnerships with treatment centers. Eligibility criteria required that participants were age 18 or older, able to read and write in English, had moderate to severe AUD ^[(≥4 self-reported DSM-5 symptoms)], had a goal of abstinence from alcohol, had been abstinent for 1–8 weeks, were willing to use a single smartphone, and were not exhibiting severe psychosis or paranoia.^[Defined as scores >2.2 or 2.8, respectively, on the psychosis or paranoia scales of the Symptom Checklist–90 [@derogatislBriefSymptomInventory].]

Participants completed up to 5 study visits over approximately 3 months: a screening visit, intake visit, and 3 monthly follow-up visits. At screening we collected demographic information (age, sex at birth, race, ethnicity, education, marital status, employment, and income) and clinical characteristics (DSM-5 AUD symptom count, alcohol problems [@hurlbutAssessingAlcoholProblems1992], and presence of psychological symptoms [@derogatislBriefSymptomInventory]). At intake we collected additional self-report data on abstinence self-efficacy [@mckiernanDevelopmentBriefAbstinence2011], craving [@flanneryPsychometricPropertiesPenn1999], and recent recovery efforts. 

At each monthly follow-up, we downloaded backups of participants' cellular communication metadata directly from their smartphones. Metadata included the phone number of the other party, the date and time of the communication, the origin of call or message (i.e., incoming or outgoing), whether the call was answered (voice calls only), and the duration of the call (voice calls only). During each follow-up visit, study staff identified important contacts. Contacts that participants communicated with at least twice by call or text in the past month were deemed important. For each important contact, participants answered 7 contextual questions about their type of relationship, whether they ever drank alcohol with this person, the drinking status of the contact, expectations about whether the contact would drink in their presence, recovery status of contact, level of supportiveness of contact, and affective experiences with the contact. 

While enrolled, participants completed 4 brief daily ecological momentary assessments (7-10 questions). The first item assessed alcohol use (date and time of any unreported drinking episodes). Lapse reports were verified at follow-up visits using a timeline follow-back interview. Additional sensing data streams and self-report measures were collected for the parent grant. The full study protocol is available on our Open Science Framework page ([https://osf.io/wgpz9/](https://osf.io/wgpz9/)). 


## Data Analysis Plan
Our models predicted the probability of an alcohol lapse within a 24-hour window. Predictions were generated daily at 4 a.m., beginning on participants' second study day and continuing for up to 3 months. Participants reported the date and hour of the start and end time of any alcohol use on the first item of the EMA. Prediction windows were labeled as lapse if any alcohol use was reported in the 24-hour window. In total, there were 11,507 labeled prediction windows across all participants. Positive lapse labels were underrepresented (7.5%; 861/11,507).

We filtered the data to include only communications with known context (i.e., people with whom they communicated with at least twice in a month and whom they provided self-report context about). Cellular communication features were engineered from all available data up to the start of each window. We used six scoring epochs (6, 12, 24, 48, 72, and 168 hours before the start of the prediction window) to create features. Within each scoring epoch we calculated two types of features: raw and difference features. Raw features represent the raw feature value calculated within a scoring epoch (e.g., the rate count of text messages during the 48 hours immediately preceding the start of the prediction window). Difference features capture participant-level changes from their baseline scores. Specifically, we subtracted each participant’s mean score for each feature (using all available data prior to the prediction window) from the associated raw feature (e.g., the participant’s average rate count of text messages across all time on study subtracted from the rate count in the preceding 48 hours).

The full model included 406 features from cellular communication data plus 24 numeric or dummy-coded features from baseline self-report measures. We also evaluated a comparison model that used only the baseline features. @tbl-1 details the raw predictors, feature engineering procedures, and features included in the full vs. baseline models. Other feature engineering steps performed during cross-validation included imputing missing values (median imputation for numeric features and mode imputation for nominal features) and removing zero and near-zero variance features as determined from held-in data. 

<!--I dont think I understand context enough from the text and table either.  We need to know how it was collected and on who (only frequent contacts).   We then need to know that numbers are linked to these contacts so you can count them but that the features that dont use context are for all contacts, regardless if they are a frequent person-->
<!--KW: I tried to make this more clear in procedure section and this section. Let me know if it does not come through. We filtered out data from unknown contacts prior to feature engineering so those logs are not included even in generic features-->

Candidate model configurations differed by algorithm (elastic net, random forest, XGBoost), outcome resampling method, and hyperparameter values. The best configuration for each model was selected using 6 repeats of participant-grouped 5-fold cross-validation. Our performance metric was area under the receiver operating curve (auROC). Folds were stratified so that all folds contained comparable proportions of individuals who lapsed frequently (i.e., 10+ times).

We evaluated model performance with a Bayesian hierarchical generalized linear model. Posterior distributions with 95% credible intervals (CI) were estimated from the 30 held-out test sets using weakly informative, data-dependent priors to regularize and reduce overfitting.^[Residual SD ~ normal(0, exp(2)); intercept (centered predictors) ~ normal(2.3, 1.3); window-width contrasts ~ normal(0, 2.69); covariance ~ decov(1,1,1,1).] Random intercepts were included for repeat and fold (nested within repeat). auROCs were logit-transformed and regressed on model type to estimate the probability that model performances differed systematically. 

Our best performing models used an elastic net algorithm. We quantified feature importance by examining the retained features (i.e., coefficient value > 0) in the full model and ordering them by absolute coefficient value. These values provide an estimate of the direction and magnitude of association between each predictor and the outcome, conditional on the other features retained. All our annotated analysis scripts are publicly available on our study website ([https://jjcurtin.github.io/study_messages/](https://jjcurtin.github.io/study_messages/)).

{{< embed notebooks/mak_tables.qmd#tbl-1 >}} 


## Ethical Considerations
All procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study #2015-0780). All participants provided written informed consent.


# Results

## Participants
We screened 192 participants. Of these, 169 enrolled and 154 completed the first follow-up visit. Data from 1 participant was excluded due to not reporting a goal of abstinence and lapsing multiple times a day every day on study. Data from 1 participant was excluded due to evidence of careless responding. Data from 1 participant was excluded due to poor compliance with EMA resulting in questionable lapse labels. Data from 7 participants were excluded due to poor compliance providing communication data (i.e., deleting logs prior to download and/or not providing context information about important contacts). The final analytic sample included 144 participants. @tbl-2 provides the demographic characterization of our sample. 56% of participants reported at least one lapse while on study. 

{{< embed notebooks/mak_tables.qmd#tbl-2 >}} 

## Communications
Participants had an average of 26 important contacts (range 2-113) that were contextualized with self-report information. We obtained a total of 375,912 contextualized communications across participants. Participants had, on average, 2,610 contextualized communications (range = 109-14,225) averaging to about 33 communications per day (range 3-278). 

## Model Evaluation
The median posterior auROC for the full model was `r round(subset(pp_perf, model == "full model")$pp_median, 2)`, with relatively narrow 95% CI ([`r round(subset(pp_perf, model == "full model")$pp_lower, 2)`, `r round(subset(pp_perf, model == "full model")$pp_upper, 2)`]) that did not contain .5. This provides strong evidence that the model is capturing signal in the data. The final model retained 13 features (@fig-1). The top four were baseline measures of abstinence confidence, having a goal of abstinence, abstinence self-efficacy when experiencing negative affect, and craving. Communication frequency with people unaware of the individual's recovery goals also emerged as an important feature associated with increased lapse risk.

We evaluated a comparison model to assess the incremental predictive value of cellular communication features beyond baseline measures. The baseline model retained 5 features and achieved performance nearly identical to the full model (median auROC = `r round(subset(pp_perf, model == "baseline model")$pp_median, 2)`, 95% CI [`r round(subset(pp_perf, model == "baseline model")$pp_lower, 2)`, `r round(subset(pp_perf, model == "baseline model")$pp_upper, 2)`]). The median difference in auROC between the full and baseline models was less than .01, providing no evidence (52% probability) that the full model performed better than the baseline model.
<!--I think this is the probasbilityh that the full model performed better than the baseline model.  Bit surprised that its even .52 given how close they auROCs are. Are you sure about that?-->
<!--KW: yes you are right I updated the wording. As far as probability being .52 this is accurate. .50 would mean its essentially a coin flip for which model performs better (i.e., they pretty much perform the same) so .52 seems expected to me.  If probability was 0 that would actually mean the baseline model was performing better than the full model-->

<!-- 
Retained communication features:
p168.l0.dratecount.phone_number,
p72.l0.rratecount.drink_status.NonDrinker,
p168.l0.rratecount.drink_status.Drinker,
p168.l0.rratecount.support_status.Dont Know,
p168.l0.rratecount.contact_experience.Unpleasant,
p168.l0.rratecount.cont_type.friend   
-->

{{< embed notebooks/mak_figures.qmd#fig-1 >}} 




# Discussion

Our model achieved fair performance, with an auROC of `r round(subset(pp_perf, model == "full model")$pp_median, 2)`, indicating that some predictive signal was present. However, it did not offer incremental value beyond a baseline model that included only demographic and self-report measures. Consistent with this, the four most important predictors in our model were all self-report variables: abstinence confidence, abstinence goal, negative affect efficacy, and craving. 

Nonetheless, several communication features were retained in the final model with moderately sized coefficients. These included communications with people unaware of the participant's recovery status, non-drinkers, friends, and individuals who were unpleasant to interact with. In contrast, raw counts of calls and text messages and call durations were not retained in the final model. This implies that the quantity of communication may be less informative than the quality and social significance. Future research may benefit from collecting richer contextual data about communication contacts to better understand the social dynamics contributing to lapse risk. 

Even with highly contextualized communication data, however, prediction may be limited by data sparsity. Many participants had few daily communications, and some had extended periods with no recorded interactions at all. Our study design may have further contributed to this limitation. We collected only phone and SMS text communications through the native smartphone app. In recent years, many individuals use private messaging apps (e.g., WhatsApp, Signal) or social media platforms (e.g., Facebook Messenger, Instagram) as their primary communication method [@mcdowellPreferencesAttitudesDigital2025]. Therefore, our dataset likely missed a substantial portion of participants' communications. Notably, the communication features that were retained in the final model were scored over the longest scoring epochs (72 and 168 hours), suggesting that when more data are available (i.e., more communications) these features may be become more important.<!--KW: John, I added this sentence but let me know if you think it needs more explanation or doesn't fit here.--> Future studies could explore whether increasing communication data from these alternative platforms yield stronger predictive signals.

We cannot entirely dismiss the potential value of cellular communication data for risk prediction. For example, researchers have successfully incorporated communication data into models with other sensing data (e.g., accelerometer, geolocation, and device usage) to detect current [@baeDetectingDrinkingEpisodes2017] and predict future [@baeLeveragingMobilePhone2023] heavy drinking episondes in non-treatment seeking young adult populations. However, even in these instances, the unique contribution of cellular communications is unclear. Some communication features, such as outgoing call duration and the number of outgoing calls emerged in the top 20 important features for detecting current drinking episodes. Conversely, when predicting future drinking episodes, no communication features appeared in the top 20. Other sensing methods, like geolocation and accelerometer data, appeared to be more robustly important for both detection and prediction. 

Other practical challenges in collecting call and text message data further limit the feasibility of this sensing method. For example, we obtained participants' cellular communication data by downloading backups of their communication logs in person during their monthly follow-up visits. However, Apple heavily restricts apps in its app store from accessing call and text message data, making real-time sensing of communications challenging (if not impossible) for IOS users. We conclude that other forms of social interaction characterization (e.g., engineering time spent with supportive contacts from geolocation data) are more worthwhile to pursue in future research.



\newpage
