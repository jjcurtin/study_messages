---
title: "Results for Communications Study"
author: "Kendra Wyant"
date: "`r lubridate::today()`"
number-sections: true
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 4
editor_options: 
  chunk_output_type: console
execute:
  echo: false
html-table-processing: none
---

```{r}
#| message: false
#| warning: false
#| echo: false

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(source("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true"))
suppressPackageStartupMessages(library(tidyposterior))
library(kableExtra, exclude = "group_rows")
library(patchwork)

theme_set(theme_classic())

path_models <- format_path(str_c("risk/models/messages"))
path_shared <- format_path("risk/data_processed/shared")

ci <- read_csv(here::here(path_models, "pp_perf_tibble.csv"), 
                                 show_col_types = FALSE)
pp_fair_full <- read_csv(here::here(path_models, "pp_fairness_full.csv"), 
                                 show_col_types = FALSE)

ci_fair_full <- read_csv(here::here(path_models, "pp_fairness_contrast_full.csv"), 
                                 show_col_types = FALSE)

pp_fair_meta <- read_csv(here::here(path_models, "pp_fairness_meta.csv"), 
                                 show_col_types = FALSE)

ci_fair_meta <- read_csv(here::here(path_models, "pp_fairness_contrast_meta.csv"), 
                                 show_col_types = FALSE)

model_full <- read_rds(here::here(path_models, "best_model_v17_kfold_full.rds"))
model_meta <- read_rds(here::here(path_models, "best_model_v17_kfold_meta.rds"))
model_passive <- read_rds(here::here(path_models, "best_model_v17_kfold_passive.rds"))
```


## Outline

The results are presented in the following way:

1. Evaluation of the full model 

    - This model uses all meta and context features plus baseline characteristics
    - Model performance, feature importance, fairness
    - I'm thinking we could run a model comparison between the full model and a baseline model (i.e., a model that only includes baseline features) - have not done this yet.

3. Evaluation of a contextualized metadata only model (i.e., no baseline features)

    - Model performance, feature importance, fairness
    - I'm thinking we could run a model comparison between the contextualized metadata only model and a model with only passively collected metadata - have not done this yet.


So to summarize there were a total of four models fit (2 primary models and 2 secondary models for comparison):

- full model
- baseline model
- metadata model (w/ context)
- passive metadata model

The best model was selected from 3 algorithms: glmnet, xgboost, random forest.   

All of my best models were glmnet!

## Full Model Evaluation 

### Performance

Median Bayesian posterior probability and credible intervals.    

*Note: my raw auroc was .72 so the Bayesian priors hurt me in this case, but I think the Bayesian estimate is more realistic.*

```{r}
#| echo: false 

ci |> 
  slice_head(n = 1)
```


### Feature Importance

*Discuss with JC how to collapse categories.*
```{r}
#| code-fold: true 

model_full |> 
  tidy() |> 
  mutate(estimate = -1 * estimate) |> 
  arrange(desc(abs(estimate))) |> 
  filter(abs(estimate) > 0) |> 
  filter(term != "(Intercept)") |> 
  mutate(term = reorder(term, abs(estimate)))  |> 
  ggplot(aes(x = estimate, y = term, fill = estimate > 0)) +
  geom_col() +
  scale_fill_manual(values = c("TRUE" = "tomato", "FALSE" = "steelblue")) +
  labs(
    x = "Coefficient",
    y = NULL,
    title = "Retained Features Ordered by Importance"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```


### Fairness

Interesting here how performance is worse for White participants.    

Also noting both male sex and income were retained as top features in the model.
```{r}
#| echo: false

pp_fair_full
```


#### Contrast Table
```{r}
#| echo: false

ci_fair_full |> 
          mutate(ci = str_c("[", round(lower, 3), ", ", round(upper, 3), "]"),
                  median = as.character(round(median, 3)),
         probability = as.character(round(probability, 3))) |>
          select(contrast, median, ci, probability) |> 
          mutate(contrast = factor(contrast,
                           levels = c("female vs male",
                                      "not white vs non-hispanic white",
                                      "below poverty vs above poverty"),
                           labels = c("female vs. male",
                                      "non-White and/or Hispanic vs. non-Hispanic White",
                                      "below poverty line vs. above poverty line"))) |> 
          arrange(contrast) |> 
  rename(Contrast = contrast,
         Median = median,
         `Bayesian CI` = ci,
         Probability = probability) |> 
  knitr::kable() |> 
  kableExtra::kable_classic()
```


### Full vs. Baseline Model Comparison

Full and Baseline performance
```{r}
#| echo: false 

ci |> 
  slice_head(n = 2)
```

Contrast
```{r}

```


## Metadata Model Evaluation

### Performance

```{r}
#| echo: false 

ci |> 
  slice(3)
```

### Feature Importance

Meta model feature importance - all context!
```{r}
#| code-fold: true 

model_meta |> 
  tidy() |> 
  mutate(estimate = -1 * estimate) |> 
  arrange(desc(abs(estimate))) |> 
  filter(abs(estimate) > 0) |> 
  filter(term != "(Intercept)") |> 
  mutate(term = reorder(term, abs(estimate)))  |> 
  ggplot(aes(x = estimate, y = term, fill = estimate > 0)) +
  geom_col() +
  scale_fill_manual(values = c("TRUE" = "tomato", "FALSE" = "steelblue")) +
  labs(
    x = "Coefficient",
    y = NULL,
    title = "Retained Features Ordered by Importance"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```


### Fairness

aurocs by subgroup - looks similar to full model.
```{r}
#| echo: false

pp_fair_meta
```


#### Contrast Table

```{r}
#| echo: false

ci_fair_meta |> 
          mutate(ci = str_c("[", round(lower, 3), ", ", round(upper, 3), "]"),
                  median = as.character(round(median, 3)),
         probability = as.character(round(probability, 3))) |>
          select(contrast, median, ci, probability) |> 
          mutate(contrast = factor(contrast,
                           levels = c("female vs male",
                                      "not white vs non-hispanic white",
                                      "below poverty vs above poverty"),
                           labels = c("female vs. male",
                                      "non-White and/or Hispanic vs. non-Hispanic White",
                                      "below poverty line vs. above poverty line"))) |> 
          arrange(contrast) |> 
  rename(Contrast = contrast,
         Median = median,
         `Bayesian CI` = ci,
         Probability = probability) |> 
  knitr::kable() |> 
  kableExtra::kable_classic()
```

### Metadata model (w/context) vs. passive metadata model comparison

Performance for metadata and passive models
```{r}
#| echo: false 

ci |> 
  slice(3:4)
```


Contrast
```{r}

```


### Additional notes

Not really a clear intuitive pattern for passive meta feature importance - maybe once categories are condensed. There were about 200 features retained so these are just top ones.
```{r}
#| code-fold: true 

model_passive |> 
  tidy() |> 
  mutate(estimate = -1 * estimate) |> 
  arrange(desc(abs(estimate))) |> 
  filter(abs(estimate) > 0.4) |> 
  filter(term != "(Intercept)") |> 
  mutate(term = reorder(term, abs(estimate)))  |> 
  ggplot(aes(x = estimate, y = term, fill = estimate > 0)) +
  geom_col() +
  scale_fill_manual(values = c("TRUE" = "tomato", "FALSE" = "steelblue")) +
  labs(
    x = "Coefficient",
    y = NULL,
    title = "Retained Features Ordered by Importance"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```