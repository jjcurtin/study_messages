{
  "hash": "850b2c2fee256c5b63ddb4dead178709",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Leveraging SMS Messages to Predict Near Real-Time Alcohol Lapse in AUD Patients\nauthor:\n  - name: Coco Yu\n    email: jyu274@wisc.edu\n    orcid: 0000-0002-7731-0563\n    corresponding: false\n    #roles: []\n    affiliations:\n    - Department of Psychology, University of Wisconsin-Madison\n  - name: John J. Curtin \n    orcid: 0000-0002-3286-938X\n    corresponding: true\n    email: jjcurtin@wisc.edu\n    #roles:\n      #- Project administration\n      #- Software\n      #- Visualization\n    affiliations:\n    - Department of Psychology, University of Wisconsin-Madison\nkeywords:\n  - Substance use disorders\n  - Text sensing\n  - Machine learning\nabstract: |\n  AUD remains a prevalent, lasting and costly problem in the United States, yet few receive treatment due to barriers. Smart Digital Therapeutics (DTx) are emerging as a promising tool to address these barriers. They can potentially provide continuous risk monitoring and individualized support through personal sensing and algorithms. This study leverages SMS messages both three days and one week preceding a lapse episode to predict next-day alcohol lapse. We recruited 138 participants (65 males; 121 Whites non-Hispanic) in early recovery with a goal of abstinence. Self-reported alcohol use and text messages were obtained during a 3-month period. We enginnered features from LIWC and trained models with XGBoost. We used grouped, nested cross-validation to select the best model configuration. The median auROC was .53 (95% CI [.52, .54]) for the best model across the 300 folds in the inner loop. Our model performed consistently worse for people from different underprivileged groups. We further calculated SHAP values and discovered that social processes, social behaviors and second person pronoun emerge as the most important features. Our study suggests that the LIWC model fails to capture sufficient signal from SMS messages to be clinically useful. Our next step is to use other natural language processing methods to do feature engineering and compare the results with this baseline LIWC model. Future studies should also consider other data sources to improve model performance.\n#plain-language-summary: |\n  #To be filled in.\n#key-points:\n  #- Take away point 1 \n  #- Take away point 2\ndate: last-modified\nciteproc: true\ncsl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\nbibliography: messages.bib\n#citation:\n  #container-title: To be filled in. \nnumber-sections: false \ntbl-cap-location: bottom\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\noptions(conflicts.policy = \"depends.ok\")\n\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(lubridate)\n# library(janitor, include.only = c(\"tabyl\", \"clean_names\"))\nlibrary(here, include.only = c(\"here\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nhere() starts at /Users/yujiachen/Desktop/github/study_messages\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(yardstick, exclude = \"spec\")\n\ntheme_set(theme_classic())\n\ndevtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true\")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nℹ SHA-1 hash of file is \"a58e57da996d1b70bb9a5b58241325d6fd78890f\"\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# devtools::source_url(\"https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true\")\n\npath_shared <- format_path(str_c(\"studydata/risk/data_processed/shared\"))\npath_messages <- format_path(str_c(\"studydata/risk/data_processed/messages\"))\npath_models <- format_path(str_c(\"studydata/risk/models/messages\"))\n```\n:::\n\n\n\n\n\n# Introduction\n\nAUD remains a prevalent, lasting and costly problem in the United States. According to 2022 NSDUH [@nationalinstituteonalcoholabuseandalcoholismAlcoholTreatmentUnited2023; @nationalinstituteonalcoholabuseandalcoholismAlcoholUseDisorder2023], an estimate of 29.5 million (10.5% of the population) individuals aged 12 or older had AUD in 2022, which is consistent with the estimates for 2021. AUD problems have also brought huge economic burdens to the U.S., where excessive alcohol use has cost \\$223.5 billion in 2006 and \\$249 billion in 2010 [@sacks2010NationalState2015a].\n\nDespite the high prevalence of AUD, limited number of individuals receive alcohol treatments. Only a small portion (7.6%) of individuals aged 12 or older with AUD acquired alcohol use treatment in the past year [@nationalinstituteonalcoholabuseandalcoholismAlcoholTreatmentUnited2023; @nationalinstituteonalcoholabuseandalcoholismAlcoholUseDisorder2023]. This low treatment acquisition rate is even more pronounced for individuals from disadvantaged groups. Disparities in treatment-seeking behavior based on race, gender, and age are evident [@abrahamAvailabilityMedicationsTreatment2020; @dibartoloAlcoholUseDisorder2017; @youngDifferencesPerceptionsPractices2018; @kaufmannTreatmentSeekingBarriers2014; @schulerPerceivedBarriersTreatment2015; @verissimoInfluenceGenderRace2017]. Only 6.6% of Black or African-American people, 3.8% of people of two or more races, and 4.8% of Hispanic or Latino people with past-year AUD received treatments [@nationalinstituteonalcoholabuseandalcoholismAlcoholTreatmentUnited2023; @nationalinstituteonalcoholabuseandalcoholismAlcoholUseDisorder2023]. Women are less likely to obtain treatments compared to men, even if they have equivalent level of perceived need for help [@gilbertGenderDifferencesUse2019b].\n\nSeveral obstacles hinder people's utilization of treatment options. Previous studies have identified that AUD patients face financial barriers [@kaufmannTreatmentSeekingBarriers2014; @schulerPerceivedBarriersTreatment2015], lack of knowledge or awareness [@williamsBarriersFacilitatorsAlcohol2018; @probstAlcoholUseDisorder2015; @mayBarriersTreatmentAlcohol2019], social stigma [@sedarousCultureStigmaInequities2023; @finnPerceivedBarriersSeeking2023; @wallhedfinnAlcoholConsumptionDependence2014; @mayBarriersTreatmentAlcohol2019], and geographical barriers [@gregoryFirstlineMedicationsOutpatient2022] for accessing care. Individuals with disadvantaged demographic background suffer from even heightened barriers to treatment resources. Women with high severity of alcohol use face larger fear of stigma compared to their male counterparts [@finnPerceivedBarriersSeeking2023]. Provider availability in rural areas, fear of stigma, and economic hardship all contribute to racial, gender and age disparities in treatment seeking [@abrahamAvailabilityMedicationsTreatment2020; @dibartoloAlcoholUseDisorder2017; @youngDifferencesPerceptionsPractices2018; @kaufmannTreatmentSeekingBarriers2014; @schulerPerceivedBarriersTreatment2015; @verissimoInfluenceGenderRace2017].\n\n## DTx and smart DTx\n\nDTx is an effective and accessible supplemental tool that may partially address extant challenges AUD patients face. DTx are evidence-based health software that deliver assessments, interventions, and other supports to patients to prevent, or manage a disease or disorder. They can be utilized to provide continuing care for AUD patients after treatment. Available evidence suggests they are generally effective for mental health conditions including AUD and demonstrate clear clinical advantages [@philippeDigitalHealthInterventions2022; @lecomteMobileAppsMental2020; @gustafsonSmartphoneApplicationSupport2014b]. Further, DTx may provide better access for hard-to-reach populations, including those from socially marginalized group who encounter increased barriers to access continuing support. They can benefit patients from rural areas with low provider availability as they are accessible remotely via mobile phones [@bucciTheyAreNot2019; @jacobsonUsingDigitalTherapeutics2023]. DTx also have potentials to relieve barriers to seek professional help resulting from stigma concerns as they can facilitate anonymity and shorten the need for in-person interactions. They are of lower costs compared to in person care and have the capacity to be scalable.\n\nThe concept of \"smart\" DTx is emerging in recent years allowing us to expand the benefits of DTx. Smart DTx comprises two key components. First, they rely on personal sensing methods to collect data. Exemplary measures include EMA, which actively prompts users to complete surveys and can encompass desired questions on moods, social relationships, stressful events, etc. Other relatively lower-burden sensing approaches include the measurement of geolocation, phone call logs and text messages collection. Those measures continuously and passively gather information. They can thus provide longitudinal data that can be temporally precise. Combined with self-reports that acquire contextual information (e.g., levels of support users can obtain from their frequently contacted people), they constitute rich data source for smart DTx.\n\nSecond, smart DTx incorporates the rich dataset extracted from personal sensing to build machine learning algorithms. The goal of such models is to identify *who* are at heightened risk for alcohol lapses, *when* they will lapse, and *why* they are at increased risk. Through the embedded algorithms, smart DTx can achieve two tasks: 1) continuous AUD relapse risk-monitoring; and 2) individualized clinical support when needed.\n\nTo enhance the effectiveness of smart DTx, those predictive modeling should be accurate and temporally precise. In other words, they should be capable of detecting increased lapse risks in a timely manner. These predictions allow windows for subsequent support to prevent lapses targeting a specific person. Notably, these functions are very important given the dynamic nature of alcohol relapse. Some relapse factors (e.g., drinking partner, drinking behavior) are situated and can be fluctuating over time. They usually occurs before an episode of alcohol use [@chihPredictiveModelingAddiction2014a]. They can be elusive to therapists' attention due to infrequent visits, highlighting importance of sustained monitoring to prevent lapses. AUD is a chronic, dynamic and temporally varying disease where patients face constant challenges of relapsing after abstinence [@scottPathwaysRelapseTreatment2005; @anderssonRelapseInpatientSubstance2019; @witkiewitzModelingComplexityPosttreatment2007a]. This requires constant monitoring over time and timely intervention when necessary.\n\nImportantly, the machine learning models should also predict well among individuals from disadvantaged groups. One of the key benefits of DTx is that they partially address barriers utilizing professional help by providing 24/7/365, affordable, personalized support. They are particularly beneficial to marginalized groups who have low rates of utilizing treatment options due to these treatment barriers. However, they might exacerbate health inequity if embedded algorithms perform relatively worse for less privileged groups. To improve algorithm fairness among different demographic subgroups, the models can incorporate a diverse representation of sample to complete model training. On the other hand, the models should derive more \"fair\" features. For example, models might be biased if they leverage theory-driven features only. These features might be biased against disadvantaged groups because they depend on decades of research on White males.\n\nTo develop such algorithms, we must first identify a clinically relevant outcome to predict. This outcome should be clearly and precisely defined across individuals, easy to measure, and with high temporal precision. Relapse, which usually refers to the return of a symptomatic behavior, is hard to quantify due to its multidimensional nature and temporal coarseness [@millerWhatRelapseFifty1996]. One potential conceptualization of alcohol relapse is linked to problems of use. Nonetheless, negative consequences are multifaceted and can therefore be burdensome to collect. It is also unclear what the onset of problems are. Another possible outcome is quantity of alcohol use. However, this measure is not temporally precise because there will be a time lag between onset of drinking and the last drink completed. Levels of drinking might also mean differently for individuals with different AUD severity, which makes it hardly generalizable across individuals. Alternatively, in this study, we utilize lapse (i.e., a single episode of alcohol use) as our primary outcome variable. Lapses are easy to define, have a clear onset, and are also clinically meaningful. They can serve as an early warning sign of failure to sustain a desired behavioral change [@chungRelapseAlcoholOther2006a; @marlattRelapsePreventionMaintenance2005a]. Research has also shown that initial lapse and frequent lapses are the associated with enhanced risk of relapse [@witkiewitzRelapsePreventionAlcohol2004b; @hogstrombrandtPredictionSingleEpisodes1999a].\n\nNext, we need to determine what inputs to use for our prediction model. The features should be easy to measure and feasible. The widespread availability of smartphones has rendered constant data collection with DTx attainable. Data collection procedure should not be unduly burdensome to users to ensure that sensing is sustainable. Previous research has established satisfying acceptability of active self-report measures such as EMA and even more willingness to use passive sensing measures [@wyantMachineLearningModels2024]. The features should also be well-validated. For example, GPS tracking has high accuracy in locating individuals and is temporally precise. Text sensing captures precisely the interactions between users and other individuals via SMS.\n\nSmart DTx algorithms should also incorporate features that are interpretable and can map on to current interventions. Exemplary features that are highly interpretable include those derived from the theory-driven approach. These features are easy to interpret and closely align with established therapies. For example, features related to social relationship might have important implications for family or marital counseling. Affect state features might be informative of emotion-focused therapy. In addition to selecting interpretable features during data training, computational methods can also be used to enhance model interpretability by analyzing feature importance. For instance, we can examine global feature importance to determine which features contribute the most to predict lapses across individuals. Further, examining local feature importance (i.e., features influencing a single observation) in these models might also be helpful for model interpretation. They have benefits of identifying the factors that contribute to lapse risk for any specific person and moment in time.\n\n## Recent progress in smart DTx\n\nMachine learning models leveraging ecological momentary assessment (EMA) measures have performed relatively well to predict goal-inconsistent alcohol use (e.g., lapses). Our group developed an XGBoost machine learning model using self-reported craving, affect, efficacy, risky situations, stressful events, pleasant events to predict alcohol lapses in the next hour, day, or week [@wyantMachineLearningModels2024]. The surveys were collected up to four times daily for three months. The model achieved exceptional performance when predicting lapses for new individuals, with a mean auROC score of .89, .90 and .93 for the hour-, day-, and week-level model respectively. Global feature importance demonstrated that past alcohol use and future self-efficacy consistently contributed greatly across all models to predict lapses.\n\nNonetheless, relying on EMA measures for model building is associated with several limitations. First, constantly completing surveys makes it burdensome for real-world DTx use. Although most EMA relevant mental health research demonstrated modest compliance rates, their time windows last from two weeks to three months [@porras-segoviaSmartphonebasedEcologicalMomentary2020; @czyzEcologicalAssessmentDaily2018; @vangenugtenExperiencedBurdenAdherence2020a; @mackesy-amitiFeasibilityEcologicalMomentary2018; @hungSmartphonebasedEcologicalMomentary2016]. The study length is insufficient for real-world DTx use. As extended period of time of app use is anticipated, users' perceived burden of answering surveys is presumably larger [@mogkImplementationWorkflowStrategies2023]. This is particularly problematic as AUD is a chronic disease that requires constant risk monitoring. Although minimizing the number of items in the surveys and the frequency of prompting users to complete the surveys might help mitigate the associated burden, it can inevitably reduce the prediction precision and temporal precision of algorithms.\n\nSecond, decisions regarding what constructs to assess and what items to include to assess these constructs are limited by theory and past data. Given this, we might miss important constructs that predict lapses among individuals from groups that have been less well-studied. Including risk factors solely drawn from decades of research on White, male-dominant samples might even exacerbate health disparities when applied to DTx. Further, our current understanding of alcohol relapse precursors is not comprehensive. For example, Marlatt's proposed taxonomy characterizes high-risk situational precursors to alcohol relapse such as social pressure and positive/negative emotional state [@marlattTaxonomyHighriskSituations1996a]. Nonetheless, replication studies have found this theoretical framework to be somewhat unreliable and have low predictive validity of post-treatment outcomes [@lowmanReplicationExtensionMarlatt1996a; @stoutPredictiveValidityMarlatt1996; @kaddenMarlattRelapseTaxonomy1996]. A review study also suggests that current relapse factors are not well-understood in past research due to methodological constraints and a death of \"near real-time\" data [@mckayConceptualMethodologicalAnalytical2006b].\n\n## Incorporating SMS in smart DTx\n\nText sensing technology, which is both feasible and sustainable, represents new opportunities in DTx that might address limitations of the current active reporting approach. Since AUD is a chronic condition requiring ongoing risk monitoring over an extended period, DTx can benefit from SMS sensing because it places a low burden on users and allows for continuous data collection. Studies collecting passive data have demonstrated high acceptability from participants and higher compliance rates compared to active measures [@wyantAcceptabilityPersonalSensing2023a; @beukenhorstUsingSmartphonesReduce2022a]. Further, risk monitoring using SMS sensing is temporally sensitive to fluctuating risks. Analyzing text messages can detect potential triggers in time without actively prompting users to reflect on their feelings at the moment or report their environment.\n\nLinguistic Inquiry and Word Count (LIWC) is a well-established text analysis tool that can potentially work well to predict alcohol lapses. It counts the frequency of words that fall into different categories to analyze a variety texts [@pennebakerDevelopmentPsychometricProperties2015a]. LIWC might have good performance because they allow researchers to mine previously established robust construct of alcohol lapse precursors. It involves domain-specific knowledge of known risks for alcohol lapses. For instance, the majority of an inpatient teen sample reported initial relapse to alcohol when offered alcohol, when in negative state, and when in interpersonal conflicts [@brownCharacteristicsRelapseFollowing1989]. Other commonly found risk factors include alcohol craving [@mckayStudiesFactorsRelapse1999a; @korlakuntaReasonsRelapsePatients2012], negative affect state [@mckayStudiesFactorsRelapse1999a], cognitive factors [@mckayStudiesFactorsRelapse1999a], and interpersonal problem [@mckayStudiesFactorsRelapse1999a]. LIWC might be capable of mining those risk-relevant factors because it incorporates a dictionary of words associated with social processes, affect, and substances.\n\nModels leveraging LIWC can also benefit from the word categories that have not been explored in past research (e.g., pronouns, clout). This approach is less susceptible to our knowledge gap and might even be capable of identifying unrecognized risk factors for alcohol lapses. We might even expect less differential performance under the bottom up approach across privileged vs. unprivileged groups because the features are not drawn from past research dominated by White males with AUD and are thus potentially unbiased towards privileged groups.\n\nText analysis also offers avenues for model interpretation to yield valuable insights into treatment recommendations. LIWC can generate features that are highly interpretable and some might even relate to extant interventions. Assessing their feature importance helps us understand how features contribute to the models (i.e., which features are robust in predicting lapses). For example, global feature importance can identify robust predictors across individuals in predicting lapses. Local feature importance provides insights on what contributes to a lapse for a specific person at a specific time. They can be useful in personalized treatment recommendations.\n\n## Current Study\n\nIn the current study, we ran participants text messages over a period of three months feature engineering techniques through the LIWC program. We used generated features as inputs to models that predict alcohol lapses. We evaluated both model performance and interpretability of each distinct method. Followings are the more specific aims:\n\n**Aim 1: Train and evaluate performance of machine learning models using language features derived from LIWC to predict alcohol lapses.** We used LIWC to engineer features from raw SMS messages. For each distinct feature set derived from a variety of configurations, we trained machine learning models a contemporary statistical algorithms (XGBoost). We evaluated and statistically compared the model performance, quantified as area under the receiver operating characteristic curve (auROC).\n\n**Aim 2: Identify important features and evaluate their interpretability with respect to recommending interventions.** Model interpretation is key to providing treatment recommendations and uncovering potential causes of lapses. We used contemporary approaches to quantify feature importance (e.g., SHAP) of features within each of the NLP techniques used.\n\n**Aim 3: Examine model fairness in historically underprivileged subgroup populations.** It is also important to note that if embedded algorithms perform relatively worse for marginalized groups, their use can exacerbate rather than alleviate treatment disparities. As such, model performance between privileged vs. unprivileged groups should be carefully examined. We evaluated model performance in demographic subgroups that face excessive barriers accessing alcohol treatments or medications, including females, racial minorities, individuals living under poverty, and older population.\n\n# Approach\n\n## Overview\n\nThis study analyzed data collected from 2017-2019 from a larger grant funded by National Institute of Alcohol Abuse and Alcoholism (R01 AA024391). In this paper, we focus on methods and measures that are relevant to this study. Additional details on broader methods and the full set of measures collected are described elsewhere (see https://osf.io/w5h9y/ and [@wyantMachineLearningModels2024; @wyantAcceptabilityPersonalSensing2023a]).\n\n## Participants\n\nIndividuals in early recovery from AUD were recruited from Madison and surrounding area via social media platforms (e.g., Facebook), referrals from clinics, and television and radio advertisements. After initial phone screen, interested individuals came in-person to complete a more in-depth screening to determine their eligibility. We documented their demographic information. Inclusion criteria include that participants: 1) must be at least aged 18 or older; 2) must meet criteria for AUD with at least moderate severity (\\>four DSM-5 criteria); 3) must be abstinent from alcohol for at least one week and fewer than two months at time of intake; 4) must be able to read and write in English; 5) must be willing to use smartphone and their smartphone is compatible with our study technology. Participants were excluded if they have a lifetime history of severe and persistent mental illness. One hundred sixty-nine participants were eligible and enrolled in the study. After excluding participants who discontinued before the first follow-up session and those with low compliance rates and too few messages (\\<100 messages), we have a final sample size of 138 participants.\n\n## Procedures\n\nThe study lasted up to three months with five in-person visits (see @fig-simp). Participants completed an in-person screening visit to determine their eligibility, obtain their informed consent, and collect their demographic information and self-report measures. They then completed an intake session one week later and three follow-up visits afterwards spaced at one-month intervals. During each of the follow-up visits, a research assistant downloaded participants' SMS messages from their phone, verified reports of lapses and queried participants about any additional unreported laspes. Additional self-reported measures were obtained (see https://osf.io/w5h9y/).\n\nThroughout the course of the study, participants were expected to complete four daily EMAs that asked about their alcohol cravings, risky situations, stressful/pleasant events, etc [@wyantMachineLearningModels2024]. Notably, in the first item in the EMA survey, participants also reported their past alcohol use. Answer to this item will be used as the predicted outcome (see *Section Alcohol Lapses*).\n\n::: {#fig-simp fig-cap=\"Flowchart of in-person visits. We obtained participant demographics and alcohol use history from the screening session. SMS messages were downloaded from participant phones at each of the follow-up visit.\"}\n![](images/visits.png)\n:::\n\n## Measures\n\n### Individual Characteristics\n\nWe collected participants' individual characteristics including their demographics and their past drinking history during the screening session (see @tbl-measures).\n\n| Log Type | Measure |\n|:---|:---|\n| Demographics | Age |\n|  | Sex |\n|  | Race |\n|  | Ethnicity |\n|  | Highest Education |\n|  | Employment Status |\n|  | Total Personal Gross Income |\n|  | Marital Status |\n| Alcohol Use | Alcohol Use History |\n|  | DSM-5 Checklist for AUD |\n|  | Young Adult Alcohol Problems Test |\n|  | WHO-The Alcohol, Smoking and Substance Involvement Screening Test |\n\n: Participant self-reported measures {#tbl-measures}\n\n### Alcohol Lapses\n\nParticipants were prompted up to four times daily to report their recent alcohol use. In the first item of each daily EMA survey, dates and times of any unreported past alcohol use were obtained. Reports of past alcohol use were used as a dichotomous outcome variable (Lapse vs. No Lapse). We predicted alcohol lapses in the next 24-hour window (i.e., next day lapse prediction). Every outcome window started from 4 a.m. everyday and end 24 hours later.\n\n### SMS Messages\n\nAt each of the follow-up visits, a research assistant downloaded the participants' SMS message logs from their phone. These logs included the message type (incoming vs. outgoing), date and time sent/received, text body, contact name, whether the participants read the text or not, etc. Images and voice texts were excluded from analysis. Both group messages and one-on-one messages were obtained from participants' phones. We included only messages from/to important contacts and in group chats.\n\nFor each individual lapse window, we had predictor sets that differ in prediction window length and their analytic unit. We defined text prediction windows to be 3-day and 1-week preceding the lapse window (see @fig-window). We analyzed the two prediction windows both individually and combined (i.e., three configurations in total).\n\n::: {#fig-window fig-cap=\"Prediction Window\"}\n![](images/prediction_window.png)\n:::\n\n## Model Training\n\n### Feature Engineering\n\nText messages served as the only raw source for all feature engineering. The document sets (varying based on unit of analysis) went through a generic pre-processing step that involves removal of all emojis. Our decision to remove all emojis was due to loss of emoji data in the ios devices during back up. This study used the LIWC dictionary [@pennebakerDevelopmentPsychometricProperties2015a] and computed scores by counting frequency of words that belong to each category. We did not remove any stop words. LIWC aligns with the current alcohol relapse risk factor literature in that it examines psychometric properties including cognitive state and social processes [@brownCharacteristicsRelapseFollowing1989; @mckayStudiesFactorsRelapse1999a; @mckayStudiesFactorsRelapse1999a].\n\nWe adopted two configurations for analytic units -- individual messages and concatenated messages. In the first configuration method, we ran individual messages within the defined prediction window through LIWC. We then normalized LIWC feature scores based on the square root of word counts instead of raw word counts which was the default choice from the program. We applied the normalization on all LIWC categories other than word count, word per sentence, and the four summary measures -- analytic, clout, authenticity, and tone. We excluded the four summary categories because their raw scores were not normalized on raw word counts. Our normalization method was chosen due to the relatively short message length for individual messages. We further obtained the median and 95% percentile of normalized LIWC scores for all messages related to each lapse label.\n\nIn the other analytic unit configuration, we first concatenated all messages associated with each lapse label altogether. We then obtained LIWC results for the concatenated messages. We further adopted three configurations for normalization methods -- normalized on raw word count (i.e., default method from the program), normalized on the square root of word count, and a combination of these two.\n\n### Candidate Algorithm\n\nWe leveraged the XGBoost algorithm that differed on the above three configurations: 1) prediction window length (see *Section SMS Messages*); 2) analytic units (see *Section Feature Engineering*); and 3) normalization methods (see *Section Feature Engineering*). As we have a fairly imbalanced class labels in our dataset (see *Section Sample Distribution*), we further considered different resampling strategies including upsampling and downsampling with different ratios. Our decision to use the XGBoost algorithm was based on its two benefits. First, the algorithm has demonstrated satisfying performance in classifying lapse vs. no lapse in our lab's previous work [@wyantMachineLearningModels2024]. We can select the best model from a range of model-specific hyperparameters (mtry, tree depth, and learning rate), on top of the four above manually incorporated configurations. Second, XGBoost is well-suited to calculate Shapley values that can help us understand each feature's contributions to model output (see *Section Feature Importance*).\n\n### Model Selection\n\nWe performed grouped, nested cross-validations to perform hyperparameter tuning and select the best model configuration. The dataset was participant-grouped so that each individual was assigned to either held-in or held-out set to avoid bias of predicting participants' lapses using their own data. The nested cross-validation method uses two nested loops to divide folds. In the inner loop, held-out folds were used as a validation set for model selection. In the outer loop, held-out folds were utilized as a test set for model evaluation. We only presented validation results from the 300 sets in the inner loop in this paper because we are still working on model development. We reserved test set to evaluate the overfitting of final full model.\n\nThe primary performance metric to select the best model configuration and evaluate the model performance on the test sets was auROC. The auROC measures the probability that a randomly chosen positive case is assigned a higher score than a randomly chosen negative case. It reflects the model's ability to distinguish between the positive and negative cases across all possible thresholds. Values between .70 and .80 are considered fair, values between .80 and .90 are considered good, and values above .90 are considered excellent. Across all models that differed on the above discussed configurations, the best model was selected based on the highest median auROC across all validation sets (see *Section Machine Learning Algorithm*).\n\n## Model Evaluation\n\n### Performance Evaluation\n\nWe calculated the predicted probability scores for all our observations based on the best model configuration and then obtained the median auROC score across all 300 validation sets in the inner fold. We further performed a Bayesian hierarchical generalized linear model to estimate the posterior probability (i.e., the likelihood of achieving the results given out data) distributions of the auROCs. The two random intercepts in the models included the repeat and the fold within repeat. We reported the 95% CIs for our models' auROCs and determined if they included .5 (chance performance). If this CI included 0.5, we would conclude that our model performed no better than random guess. We also reported the probability that the model auROC is \\>.05.\n\n### Algorithmic Bias\n\nA subset of individual characteristic measures was used to evaluate model fairness on subgroups. We compared model performance among each sex, racial, age and income subgroups because the populations face increased barriers obtaining AUD treatments. Stigma among older populations and wome, and economic hardship in racial minority groups can all contribute to low treatment-seeking and alcohol treatment completion [@dibartoloAlcoholUseDisorder2017; @jacobsonRacialDisparitiesCompletion2007; @mayBarriersTreatmentAlcohol2019]. Participants younger than 55 years old were considered as a privileged group. We adopted half of median income in Madison area in 2017 as cut-off to assign participants to income groups.\n\nWe performed a Bayesian hierarchical generalized linear model that regressed the auROCs from the 300 validation sets in the inner loop as a function of group membership (privileged group vs. unprivileged group within each of above individual characteristics). We reported the 95% CI for model performance differences and examined if they included 0. If this CI did not include 0, we would conclude that our model was unfair. We also reported the probability that the difference is greater than zero.\n\n### Feature Importance\n\nWe also calculated SHAP values to interpret the results. SHAP is a game theory based method to explain how each feature influences the model output [@lundbergUnifiedApproachInterpreting2017]. It assigns importance to each feature, where a positive feature importance positively affects the model output. This methodology can be applied to any machine learning algorithm and can increase model transparency and interpretability. Local Shapley values explain factors that contribute to a single observation, and global Shapley values represent feature importance across all observations. To compute global Shapley values, we averaged the absolute value of all local Shapley values. For better understanding, we aggregated Shapley values for each LIWC category, regardless of their prediction window and normalization methods.\n\n\\newpage\n\n# Results\n\n\n\n\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# Load objects for results\nlabels <- read_csv(here::here(path_messages, \"lapses.csv\"), col_types = cols()) |> \n  mutate(day_start = as_datetime(day_start, tz = \"America/Chicago\"),\n         day_end = as_datetime(day_end, tz = \"America/Chicago\"))\n\nraw_data <- read_csv(here(path_messages, \"eda\", \"eda_raw.csv\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 313492 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): id_obs, subid, text_length\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\npred_3day <- read_csv(here(path_messages, \"eda\", \"eda_3day.csv\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 915833 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): lapse\ndbl  (2): id_obs, subid\nlgl  (1): na\ndttm (1): day_start\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\npred_1week <- read_csv(here(path_messages, \"eda\", \"eda_1week.csv\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 2086041 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): lapse\ndbl  (2): id_obs, subid\nlgl  (1): na\ndttm (1): day_start\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ntxt_length_by_id <- raw_data |> \n  group_by(subid) |> \n  summarize(\n    mean_length = mean(text_length),\n    median_length = median(text_length),\n    min_length = min(text_length),\n    max_length = max(text_length)\n  )\n\nstats_ind <- read_csv(here(path_messages, \"eda\", \"eda_liwc_ind.csv\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 468 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): skim_type, skim_variable\ndbl (5): complete_rate, numeric.sd, numeric.p0, numeric.p50, numeric.p100\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nstats_cat <- read_csv(here(path_messages, \"eda\", \"eda_liwc_cat.csv\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 468 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): skim_type, skim_variable\ndbl (5): complete_rate, numeric.sd, numeric.p0, numeric.p50, numeric.p100\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\naurocs <- read_csv(\"_csv/aurocs.csv\")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 300 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): inner_split_num, outer_split_num, auroc\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nprobs <- read_rds(here::here(path_models, \n                             str_c(\"inner_preds_\", \"v1\", \"_\", \n                                   \"nested_1_x_10_3_x_10\", \".rds\")))\npp <- read_rds(here(path_messages, \"pp\", \"pp_auroc.rds\"))\nfairness <- read_csv(\"_csv/subgroup_comparison.csv\")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 4 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): group\ndbl (4): median, lower, upper, prob\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n\n\n## Demographics\n\nThe final sample includes 138 participants. On average, participants are 39.60 years old (SD = 11.17, range = 21-66). 65 (47.10%) participants are females, 121 (87.68%) are White/Caucasian, 3 (2.17%) are American Indiain/Alaska Native, 2 (1.45%) are Asian, 6 (4.35%) are Black/African American, and 6 (4.35%) are Other/Multiracial. See @tbl-demographics for more details on participant demographics (education, employment, income, marital status) and their patterns of alcohol use.\n\n|   | N | \\% | M | SD | Range |\n|:--------------------------------------------------------------------|:--:|:--:|:--:|:--:|:--:|\n| **Age** | — | — | 39.60 | 11.17 | 21-66 |\n| **Sex** |  |  |  |  |  |\n| *Female* | 73 | 52.90% | — | — | — |\n| *Male* | 65 | 47.10% | — | — | — |\n| **Race** |  |  |  |  |  |\n| *White/Caucasian* | 121 | 87.68% | — | — | — |\n| *American Indian/Alaska Native* | 3 | 2.17% | — | — | — |\n| *Asian* | 2 | 1.45% | — | — | — |\n| *Black/African American* | 6 | 4.35% | — | — | — |\n| *Other/Multiracial* | 6 | 4.35% | — | — | — |\n| **Ethnicity** |  |  |  |  |  |\n| *Mexican, Mexican American, Chicano* | 3 | 2.17% | — | — | — |\n| *Another Hispanic, Latino, or Spanish Origin* | 1 | 0.72% | — | — | — |\n| *Not Hispanic, Latino, or Spanish Origin* | 134 | 97.10% | — | — | — |\n| **Highest Education** |  |  |  |  |  |\n| *2-Year degree* | 13 | 9.42% | — | — | — |\n| *Advanced degree* | 23 | 16.67% | — | — | — |\n| *College degree* | 54 | 39.13% | — | — | — |\n| *High school or GED* | 11 | 7.97% | — | — | — |\n| *Less than high school or GED degree* | 1 | 0.72% | — | — | — |\n| *Some college* | 36 | 26.09% | — | — | — |\n| **Employment** |  |  |  |  |  |\n| *Disabled* | 4 | 2.90% | — | — | — |\n| *Employed* | 96 | 69.57% | — | — | — |\n| *Full-time student* | 7 | 5.07% | — | — | — |\n| *Homemaker* | 1 | 0.72% | — | — | — |\n| *Other, not otherwise specified* | 8 | 5.80% | — | — | — |\n| *Retired* | 6 | 4.35% | — | — | — |\n| *Temporarily laid off, sick leave, or maternity leave* | 3 | 2.17% | — | — | — |\n| *Unemployed* | 13 | 9.42% | — | — | — |\n| **Total Personal Gross Income** | — | — | 36,494.41 | 32,149.41 | 0-200,000 |\n| **Marital Status** |  |  |  |  |  |\n| *Divorced* | 40 | 29.90% | — | — | — |\n| *Married* | 30 | 21.74% | — | — | — |\n| *Never Married* | 61 | 44.20% | — | — | — |\n| *Separated* | 5 | 3.62% | — | — | — |\n| *Widowed* | 2 | 1.45% | — | — | — |\n| **Alcohol Use History** |  |  |  |  |  |\n| *Age of First Drink Without Family* | — | — | 14.62 | 2.98 | 6-24 |\n| *Age of First Regular Drink* | — | — | 19.39 | 6.05 | 11-53 |\n| *Age of First Drinking Problem* | — | — | 27.62 | 9.42 | 15-60 |\n| *Age of First Quit Attempt* | — | — | 31.02 | 10.09 | 15-61 |\n| *Number of Quit Attempts* | — | — | 5.98 | 9.83 | 0-100 |\n| *Types of Programs or Services Used* |  |  |  |  |  |\n|  *Long-Term Residential Treatment (more than 6 months)* | 6 | 4.35% | — | — | — |\n|  *Short-Term Residential Treatment (less than 6 months)* | 41 | 29.71% | — | — | — |\n|  *Outpatient Treatment* | 64 | 46.38% | — | — | — |\n|  *Individual Counseling* | 91 | 65.94% | — | — | — |\n|  *Group Counseling* | 57 | 41.30% | — | — | — |\n|  *Alcoholics Anonymous/Narcotics Anonymous* | 84 | 60.87% | — | — | — |\n|  *Other* | 38 | 27.54% | — | — | — |\n| *Even Taken Prescribed Medication* | 53 | 38.41% | — | — | — |\n| *Number of Days per Week Consumed Any Alcohol* | — | — | 5.24 | 1.81 | 1-7 |\n| *Number of Days per Week Consumed 6 or More Alcoholic Drinks in One Day* | — | — | 3.94 | 2.07 | 0-7 |\n| *Number of Alcoholic Drinks per Day on Days Drinked* | — | — | 7.43 | 4.31 | 1-25 |\n| **DSM-5 Symptom Count** | — | — | 8.89 | 1.84 | 4-11 |\n| **Young Adult Alcohol Problems Test** | — | — | 19.90 | 4.60 | 6-27 |\n| **Past 3-Month Drug Use (WHO-The Alcohol, Smoking and Substance Involvement Screening Test)** |  |  |  |  |  |\n| *Tobacco products (cigarettes, chewing tobacco, cigars, etc.)* | 75 | 54.35% | — | — | — |\n| *Cannabis (marijuana, pot, grass, hash, etc.)* | 64 | 46.38% | — | — | — |\n| *Cocaine (coke, crack, etc.)* | 17 | 12.32% | — | — | — |\n| *Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)* | 15 | 10.87% | — | — | — |\n| *Inhalants (nitrous, glue, petrol, paint thinner, etc.)* | 3 | 2.17% | — | — | — |\n| *Sedatives or Sleeping Pills (Valium, Serepax, Rohypnol, etc.)* | 21 | 15.22% | — | — | — |\n| *Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)* | 14 | 10.14% | — | — | — |\n| *Opioids (heroin, morphine, methadone, codeine, etc.)* | 14 | 10.14% | — | — | — |\n\n: Participant demographics and alcohol use history {#tbl-demographics}\n\n\n## Sample Distribution\n\nThe final total number of lapse labels in the dataset is 11562. 7.63% of the labels are associated with a lapse episode. On average, each participant has 83.78 labels (sd = 11.35, median = 88.00, range = 30 - 90; see @fig-lapse_count).\n\n\n\n\n\n{{< embed notebooks/fig_eda_messages.qmd#fig-lapse_count >}}\n\n\n\n\n\n\n\n\nThe total number of messages in the dataset is 313492. On average, each subject has 2271.68 messages (sd = 2536.60, range = 100 - 15884). The average message length across all participants is 10.42 words (sd = 14.30, median = 7.00, range = 1 - 1266). On average, each participant has a mean message length of 10.87 (sd = 3.97, range = 5.25 - 38.91).\n\n\n\n\n\n{{< embed notebooks/fig_eda_messages.qmd#fig-raw >}}\n\n\n\n\n\n\n\n\nOn average, each lapse label has 79.21 messages (sd = 112.13, median = 40.00, range = 1 - 1414) during the 3-day prediction window. 11.92% of labels have no associated messages in the previous 3 days. Each participant has an averaged 79.48 messages as predictors per label (sd = 87.12, median = 50.12, range = 2.97 - 529.45). On average, each participant's data missingness is 12.28% (sd = 19.42%, median = 1.14%, range = 0.00% - 77.27%).\n\n\n\n\n\n{{< embed notebooks/fig_eda_messages.qmd#fig-3day >}}\n\n\n\n\n\n\n\n\nOn average, each lapse label has 180.42 messages (sd = 241.30, median = 96.00, range = 1 - 2866) during the 1-week prediction window. 7.83% of labels have no associated messages in the previous week. Each participant has an averaged 181.23 messages as predictors per label (sd = 199.32, median = 113.47, range = 6.75 - 1200.76). On average, each participant's data missingness is 8.14% (sd = 16.23%, median = 0.00%, range = 0.00% - 72.73%).\n\n\n\n\n\n{{< embed notebooks/fig_eda_messages.qmd#fig-1week >}}\n\n\n\n\n\n\n\n\n## LIWC Features\n\nWe obtained LIWC scores from 117 linguistic categories. These categories include total word count, number of words per sentence, the four summary categories (analytic, clout, authentic, and tone), and other linguistic categories such as social and pronouns. Notably, LIWC-22 now incorporates a *health* dimension that includes phrases related to illness, wellness, mental health (diagnoses or behaviors), and substances. For each unit of analysis, we had 468 engineered features after normalization and aggregation (see *Section Feature Engineering*).\n\nWhen our unit of analysis was on individual messages (see @tbl-panel and @fig-liwc_ind), the median of all median LIWC feature scores excluding the six unnormalized categories within each 3-day prediction window ranged from 0.00 to 2.45 (median = 0.00, sd = 0.34). The median of all median LIWC feature scores excluding the six unnormalized categories within each 1-week prediction window ranged from 0.00 to 2.45 (median = 0.00, sd = 0.35). The max of all median LIWC feature scores excluding the six unnormalized categories within each 3-day prediction window ranged from 0.00 to 11.58 (median = 0.80, sd = 1.60). The max of all median LIWC feature scores excluding the six unnormalized categories within each 1-week prediction window ranged from 0.00 to 8.89 (median = 0.63, sd = 1.25).\n\nThe median of all 95% percentile LIWC feature scores excluding the six unnormalized categories within each 3-day prediction window ranged from 0.00 to 4.74 (median = 0.30, sd = 0.70). The median of all 95% percentile LIWC feature scores excluding the six unnormalized categories within each 1-week prediction window ranged from 0.00 to 4.89 (median = 0.34, sd = 0.72). The max of all 95% percentile LIWC feature scores excluding the six unnormalized categories within each 3-day prediction window ranged from 0.36 to 17.51 (median = 1.15, sd = 2.40). The max of all 95% percentile LIWC feature scores excluding the six unnormalized categories within each 1-week prediction window ranged from 0.35 to 18.42 (median = 1.00, sd = 2.47).\n\n::: {#tbl-panel layout-ncol=\"1\"}\n|   | SD | Median | Min | Max |\n|---------------|---------------|---------------|---------------|---------------|\n| wc_median | 5.43 | 7.00 | 1.00 | 142.00 |\n| wc_q_95 | 20.31 | 26.40 | 1.00 | 369.05 |\n| wps_median | 2.45 | 5.00 | 0.00 | 78.00 |\n| wps_q_95 | 5.96 | 14.75 | 0.00 | 78.00 |\n| analytic_median | 17.11 | 10.19 | 1.00 | 99.00 |\n| analytic_q_95 | 19.24 | 97.20 | 1.00 | 99.00 |\n| clout_median | 32.72 | 40.06 | 1.00 | 99.00 |\n| clout_q_95 | 12.98 | 99.00 | 1.00 | 99.00 |\n| authentic_median | 19.60 | 89.39 | 1.00 | 99.00 |\n| authentic_q_95 | 10.77 | 99.00 | 1.00 | 99.00 |\n| tone_median | 17.84 | 99.00 | 1.00 | 99.00 |\n| tone_q_95 | 9.85 | 99.00 | 1.00 | 99.00 |\n\n: 3-Day Prediction Window {#tbl-ind_3day}\n\n|   | SD | Median | Min | Max |\n|---------------|---------------|---------------|---------------|---------------|\n| wc_median | 4.00 | 7.00 | 1.00 | 81.00 |\n| wc_q_95 | 17.84 | 28.00 | 1.00 | 404.00 |\n| wps_median | 2.20 | 5.00 | 1.00 | 78.00 |\n| wps_q_95 | 5.53 | 15.10 | 1.00 | 78.00 |\n| analytic_median | 14.02 | 10.19 | 1.00 | 99.00 |\n| analytic_q_95 | 14.85 | 98.59 | 1.00 | 99.00 |\n| clout_median | 30.57 | 40.06 | 1.00 | 99.00 |\n| clout_q_95 | 9.94 | 99.00 | 1.00 | 99.00 |\n| authentic_median | 15.87 | 89.39 | 1.00 | 99.00 |\n| authentic_q_95 | 7.36 | 99.00 | 1.00 | 99.00 |\n| tone_median | 13.85 | 99.00 | 1.00 | 99.00 |\n| tone_q_95 | 7.27 | 99.00 | 1.00 | 99.00 |\n\n: 1-Week Prediction Window {#tbl-ind_1week}\n\nSample Characteristics of Engineered Feature Scores (Median and 95% Quantile Scores from Individual Messages) for the Six Unnormalized Categories Within Each Prediction Window\n:::\n\n\n\n\n\n{{< embed notebooks/fig_eda_liwc.qmd#fig-liwc_ind >}}\n\n\n\n\n\n\n\n\nWhen our unit of analysis was on concatenated messages (see @tbl-panel2, @fig-liwc_cat_raw and @fig-liwc_cat_norm), the median of all raw feature scores within the 3-day prediction window ranged from 0.00 to 91.36 (median = 1.32, sd = 12.64). The median of all raw feature scores within the 1-week prediction window ranged from 0.00 to 91.30 (median = 1.36, sd = 12.63). The max of all raw feature scores within the 3-day prediction window ranged from 3.85 to 166.67 (median = 33.33, sd = 37.64). The max of all raw feature scores within the 1-week prediction window ranged from 1.82 to 166.67 (median = 27.27, sd = 35.21).\n\nThe median of all normalized feature scores within the 3-day prediction window ranged from 0.00 to 20.63 (median = 0.32, sd = 2.85). The median of all normalized feature scores within the 1-week prediction window ranged from 0.00 to 30.69 (median = 0.47, sd = 4.25). The max of all normalized feature scores within the 3-day prediction window ranged from 0.37 to 106.55 (median = 2.95, sd = 15.11). The max of all normalized feature scores within the 1-week prediction window ranged from 0.29 to 143.30 (median = 3.16, sd = 20.31).\n\n::: {#tbl-panel2 layout-ncol=\"2\"}\n|   | SD | Median | Min | Max |\n|---------------|---------------|---------------|---------------|---------------|\n| wc | 1175.22 | 512.00 | 1 | 13211 |\n| wps | 9.75 | 10.76 | 0 | 295 |\n| analytic | 16.96 | 19.05 | 1 | 99 |\n| clout | 25.37 | 50.20 | 1 | 99 |\n| authentic | 22.34 | 77.13 | 1 | 99 |\n| tone | 23.28 | 82.19 | 1 | 99 |\n\n: 3-Day Prediction Window {#tbl-cat_3day}\n\n|   | SD | Median | Min | Max |\n|---------------|---------------|---------------|---------------|---------------|\n| wc | 2484.84 | 1124.00 | 1 | 23702 |\n| wps | 8.73 | 10.91 | 1 | 241 |\n| analytic | 14.30 | 19.17 | 1 | 99 |\n| clout | 22.54 | 50.15 | 1 | 99 |\n| authentic | 19.04 | 77.10 | 1 | 99 |\n| tone | 20.76 | 82.04 | 1 | 99 |\n\n: 1-Week Prediction Window {#tbl-cat_1week}\n\nSample Characteristics of Raw LIWC Scores on Concatenated Messages Across the Six Linguistic Categories Within Each Prediction Window\n:::\n\n\n\n\n\n{{< embed notebooks/fig_eda_liwc.qmd#fig-liwc_cat_raw >}}\n\n\n{{< embed notebooks/fig_eda_liwc.qmd#fig-liwc_cat_norm >}}\n\n\n\n\n\n\n\n\n## Best Model Evaluation\n\nThe best model configuration was the one that leveraged raw LWIC scores from concatenated messages within both the 3-day and 1-week prediction window, and an upsampling technique with a ratio of 1:1. We applied the best configuration on the raw dataset and obtained an auROC score of 0.53 (see @fig-auroc for the ROC curve). We further aggregated predictions by folds, and the median score of all median auROCs across the 300 inner folds was 0.53 (sd = 0.09, range = 0.28 - 0.80; see @fig-auroc_hist). The median posterior distribution of the auROCs was 0.53 (95% CI = \\[0.52, 0.54\\]; see @fig-auroc_posterior). The probability of the posterior auROC larger than .5 was 1.00. We concluded that our model was better than chance performance.\n\n\n\n\n\n{{< embed notebooks/fig_auroc.qmd#fig-auroc >}}\n\n\n{{< embed notebooks/fig_auroc.qmd#fig-auroc_hist >}}\n\n\n{{< embed notebooks/fig_auroc.qmd#fig-auroc_posterior >}}\n\n\n\n\n\n\n\n\n## Model Fairness\n\n73 (53%) participants are females, 17 (12%) are non-White minorities, 39 (28%) earned less than half of the median income in Madison in Year 2017, and 13 (9%) aged 55 or older (see @fig-demographics). Our model performed consistently worse in unprivileged group vs. privileged group across all four demographic categories (see @fig-fairness and @tbl-fairness). The median of posterior distribution of model performance difference was 0.03 (95%CI = \\[0.01, 0.06\\]) for White people compared to People of Color. The probability of the performance for the White people higher than for People of Color was 0.99. The posterior distribution of performance differences also indicated that the model performed better in males than in females (median = 0.02, 95%CI = \\[0.01, 0.04\\], probability = 1.00), in people younger than 55 than people older than 55 (median = 0.04, 95%CI = \\[0.01, 0.07\\], probability = 1.00), and in people that have higher income than in those who have lower income (median = 0.02, 95%CI = \\[0.00, 0.04\\], probability = 0.99). We concluded that the model performed significantly worse in unprivileged groups.\n\n\n\n\n\n{{< embed notebooks/fig_demographics.qmd#fig-demographics >}}\n\n\n{{< embed notebooks/fig_fairness.qmd#fig-fairness >}}\n\n\n\n\n\n\n\n\n|   | median | lower | upper | probability |\n|---------------|---------------|---------------|---------------|---------------|\n| white vs. non_white | 0.03 | 0.01 | 0.06 | 0.99 |\n| male vs. female | 0.02 | 0.01 | 0.04 | 1.00 |\n| younger vs. older | 0.04 | 0.01 | 0.07 | 1.00 |\n| above_poverty vs. below_poverty | 0.02 | 0.00 | 0.04 | 0.99 |\n\n: Model Performance Difference across different demographic subgroups {#tbl-fairness}\n\n## Model Interpretation\n\nThe top 30 LIWC categories with the highest global shapley values were displayed in @fig-shaps. Three categories appeared to be the most robust in contributing to model output -- social processes (e.g., you, we, he, she), social behavior (e.g., said, love, say, care), and second person pronoun (e.g., you, your, yourself). The other less robust categories included total word count, clout (language of leadership), assent (e.g., yeah, yes, okay, ok), social referents (e.g., you, we, he, she), and prosocial behaviors (e.g., care, help, thank, please).\n\n\n\n\n\n{{< embed notebooks/fig_shaps.qmd#fig-shaps >}}\n\n\n\n\n\n\n\n\n\\newpage\n\n# Discussion\n\n\nOverall, our best machine learning model configuration leveraging linguistic categories from SMS messages to predict a single episode of alcohol lapse achieved an auROC score of .53. Our model was able to extract signal from text messages. Nonetheless, the .03 increase beyond chance performance is not sufficient enough for any clinical use.\n\nOur LIWC model was only planned to serve as a baseline model against which to later compare my sophisticated approaches following the FYP. Clearly, more robust features need to be engineered from SMS messages to achieve better prediction. <!--I think you want to pivot away from liwc pretty quickly.   How can you vary prediction window length furhter?  If you go any shorter than 3 day there is too much missing data.  Are you talking about longer? Not sure that makes sense either--> Our next plan is to explore other more sophisticated natural language processing (NLP) techniques to train our models. In addition to the current configurations varying in prediction window length, we might also incorporate configurations such as message types. \n\nTo improve model performance, we might consider subsetting messages based on their types. For instance, we can analyze incoming and outgoing messages individually. By distinguishing between the two, we might get a more refined understanding of communication dynamics. For example, incoming messages might indicate the level of social support one gets and outgoing messages might reflect their level of self-disclosure and willingness to seek help. In the current study, we have taken a different approach that combined the two message types together. Although it provides an overview of social interactions between participants and their contacts, it might overlook some nuanced distinctions underlying texts from those two message types. On the other hand, it is important to emphasize potential drawbacks to separate the two message types, especially given the limited number of messages (see *Section Sample Distribution*) associated with each label. It might lead to the problem of data sparsity which might even exacerbate model performance and decrease its generalizability. We might consider extended prediction window (e.g., 1-week window only) to aggregate more data.\n\nWe can further try a variety of other natural language processing (NLP) techniques for feature engineering including topic modeling, sentiment analysis, n-grams approach and word embeddings. Our next and second baseline model can be an n-grams approach. It represents bags-of-words occurrences inside a document. We can leverage term frequency, inverse document frequency and a combination of the two to mine word importance inside a document. As this method yields high-dimensional data, we might also consider combining it with dimension reduction approaches to do feature engineering. Other NLP methods we might try include topic modeling. It is an unsupervised machine learning methods to identify clusters of topics in a body of text. We can also try other Sentiment Analysis methods, beyond LIWC's native affect and emotion categories which focus mostly on valence. For example, we might infer the emotional underpinning of texts in terms of basic emotion categories (e.g., anger, fear) from SMS messages. There are also other sentiment analysis approaches outside of LIWC that can handle modifier words and provide sentence level rather than word level analysis.  We can also utilize word embeddings—vectors that represent relationships of words within a document in a lower-dimension.\n\nNote that these feature engineering techniques differ in their interpretability. Word embeddings is empirically-driven and its features that are less interpretable. <!--n grams is pretty interpretable.  Depends some on what words predict but I wouldnt offer it up an an exmple of a non-interpretable approahc--> The interpretability of n-grams can depend on the presence of words that contain high global importance. Topic modeling, though data-driven, might yield topic clusters that are interpretable. Sentiment Analysis can yield features that are highly interpretable and might map on to current interventions. Importantly, our desired predictive model should not just be the one that has the highest predictive ability. We should also examine whether our model is interpretable enough to understand intervention-relevant risk factors. This helps us determine if our model can be clinically useful.\n\nThe minimal signal we detected in SMS messages might also reflect that text messages itself is not an effective data source to predict lapses. One potential reason might include that most people do not engage in enough SMS texting behaviors that produce meaningful patterns near the time of lapsing. Some people might also not use text messaging as their main source of communication. Some might have preferences for texting in other platforms (e.g., WhatsApp, Snapchat). The lack of sufficient data points curtails machine learning models' ability to detect risk-relevant signals. To enhance model performance, we might need increased access to texts from other sources. Additionally, text messages might not convey information indicative of risk-relevant factors such as alcohol cravings [@mckayStudiesFactorsRelapse1999a; @korlakuntaReasonsRelapsePatients2012; @wyantMachineLearningModels2024]. Communications in SMS messages might involve casual and monotonous interactions which lacks specificity to effectively predict lapses.\n\nAlternatively, we can explore adoption of metadata from text messages and voice calls as predictive features. Those metadata include number of messages within a prediction window, contextual information related to the contact person, timing of messages or calls, etc. We can mine sudden changes to messaging or calling behaviors from those metadata that might signal enhanced lapse risks. For example, an increased number of late night outgoing messages might indicate a sudden mood change preceding a lapse. Note that we have already witnessed some success in our lab's previous work (see Kendra's FYP), with auROCs in mid to upper .60s.\n\nSurprisingly, despite the low performance score, we still witnessed consistent unfairness in our algorithm for unprivileged demographic subgroups. One key factor that contributed to the model biases might be the disproportionate representation of subgroups such as racial minorities. To address this limitation, we might recruit a sample with a fair distribution of people with diverse backgrounds. Importantly, however, our model still systematically misrepresented other demographic subgroups even if they were well-represented in our training data. The other explanation might be that our features were inherently biased against subgroups. People from different demographic groups might display different language styles, and LIWC might be more attuned to those from the majority groups. As such, we should further try other natural language processing techniques that incorporate more fair features. For example, topic modeling does not rely on theories built upon past, White male-dominated research and might yield more fair features.\n\nResults in this study indicate that text message might not be potentially valuable raw data source in predicting alcohol lapses in AUD patients. Although SMS messages offer the benefits of minimal burden on users, this advantage cannot compensate for their low prediction precision. EMA measures, despite their somewhat higher burden, are capable of capturing robust signals. GPS sensing, which is also not burdensome, can obtain some signals with little feature engineering (see Claire's FYP). Note that both EMA and GPS sensing are easily accessible across platforms. Nonetheless, as SMS is not currently available in ios platform, we might have diminished further interest in this data source.\n\nIn sum, our machine learning algorithm had minimal increase in performance compared to random guess. The results call for further exploration of other feature engineering techniques to build models that have higher performance, are interpretable and have low algorithmic biases.\n\n\\newpage\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}